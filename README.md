# Prompt-Fuzzing-Framework-for-LLM-Safety-Testing
Prompt Fuzzing Framework is a modular LLM safety evaluation system that mutates 186 curated seed prompts using paraphrasing, obfuscation, role injection, and model-aware jailbreak wrappers to stress-test LLaMA and Mistral models. Includes local LM Studio inference, safety classification, and a triage dashboard for benchmarking.
